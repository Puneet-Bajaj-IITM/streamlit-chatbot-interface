{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!curl https://ollama.ai/install.sh | sh # Install Ollama AI\n",
        "!nohup ollama serve & # Start Ollama AI service\n",
        "!pip install ollama # Install Ollama Python client\n",
        "!ollama pull llama2 #install model\n",
        "!pip install streamlit langchain_community\n"
      ],
      "metadata": {
        "id": "Utu9AjE0lZa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install docx2txt\n",
        "!pip install tiktoken\n",
        "!ollama pull nomic-embed-text\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "jawHcVgGVWN7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze >> requirements.txt"
      ],
      "metadata": {
        "id": "HAfXG3aSl75W"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "fN66IUljY1Ab"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community import embeddings\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
        "loader = Docx2txtLoader('About_the_founders.docx')\n",
        "data = loader.load()\n",
        "doc_splits = text_splitter.split_documents(data)\n",
        "\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=OllamaEmbeddings(model='nomic-embed-text'),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "class StreamingCallbackHandler(BaseCallbackHandler):\n",
        "    def __init__(self):\n",
        "        self.partial_output = \"\"\n",
        "\n",
        "    def on_llm_new_token(self, token, **kwargs):\n",
        "        self.partial_output += token\n",
        "        print(token, end=\"\", flush=True)\n",
        "\n",
        " # URL processing\n",
        "def process_input(question):\n",
        "    model_local = Ollama(model=\"llama2\", callbacks=[StreamingCallbackHandler()])\n",
        "\n",
        "    after_rag_template = \"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
        "    after_rag_chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | after_rag_prompt\n",
        "        | model_local\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return after_rag_chain.invoke(question)\n",
        "\n",
        "import streamlit as st\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import shelve\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "st.title(\"Streamlit Chatbot Interface\")\n",
        "\n",
        "USER_AVATAR = \"ðŸ‘¤\"\n",
        "BOT_AVATAR = \"ðŸ¤–\"\n",
        "\n",
        "# Ensure openai_model is initialized in session state\n",
        "if \"openai_model\" not in st.session_state:\n",
        "    st.session_state[\"openai_model\"] = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "\n",
        "\n",
        "# Load chat history from shelve file\n",
        "def load_chat_history():\n",
        "    with shelve.open(\"chat_history\") as db:\n",
        "        return db.get(\"messages\", [])\n",
        "\n",
        "\n",
        "# Save chat history to shelve file\n",
        "def save_chat_history(messages):\n",
        "    with shelve.open(\"chat_history\") as db:\n",
        "        db[\"messages\"] = messages\n",
        "\n",
        "\n",
        "# Initialize or load chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = load_chat_history()\n",
        "\n",
        "# Sidebar with a button to delete chat history\n",
        "with st.sidebar:\n",
        "    if st.button(\"Delete Chat History\"):\n",
        "        st.session_state.messages = []\n",
        "        save_chat_history([])\n",
        "\n",
        "# Display chat messages\n",
        "for message in st.session_state.messages:\n",
        "    avatar = USER_AVATAR if message[\"role\"] == \"user\" else BOT_AVATAR\n",
        "    with st.chat_message(message[\"role\"], avatar=avatar):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Main chat interface\n",
        "if prompt := st.chat_input(\"How can I help?\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\", avatar=USER_AVATAR):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\", avatar=BOT_AVATAR):\n",
        "        message_placeholder = st.empty()\n",
        "        full_response = \"\"\n",
        "        for response in process_input(prompt):\n",
        "            full_response += response or \"\"\n",
        "            message_placeholder.markdown(full_response + \"|\")\n",
        "        message_placeholder.markdown(full_response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "\n",
        "# Save chat history after each interaction\n",
        "save_chat_history(st.session_state.messages)\n",
        "\n"
      ],
      "metadata": {
        "id": "KChOu06Qf8fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve & # Start Ollama AI service\n",
        "!ollama pull llama2 #install model"
      ],
      "metadata": {
        "id": "Es4-CSkQh-5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "ngrok.set_auth_token('2aGUSiqfIzgKx0yh6XHaTX3G11f_5HRDp7atGGPtyxaiH91bc')\n",
        "\n",
        "port = 8501\n",
        "streamlit_process = subprocess.Popen([\"streamlit\", \"run\", \"/content/app.py\", f\"--server.port={port}\"])\n",
        "\n",
        "# Start ngrok tunnel for the Streamlit app\n",
        "ngrok_tunnel = ngrok.connect(addr=f'{port}')\n",
        "\n",
        "# Print the ngrok URL\n",
        "print(\"Ngrok Tunnel URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# Block until Streamlit process finishes\n",
        "streamlit_process.wait()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "VBCz3iuIZYmh",
        "outputId": "891e86e4-d608-4e8c-af04-af2a17ae5bb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok Tunnel URL: https://f7c4-35-204-182-23.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-86cf60394e32>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Block until Streamlit process finishes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstreamlit_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_input('Which school information are you given in context')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "6k8yF1ViZahP",
        "outputId": "7175acf9-40f6-4c87-b6f1-e93cf720d95c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document provided contains information about the department of physics at a university, including the names and qualifications of the faculty members, their experience, and the facilities available in the research center.\n",
            "\n",
            "In the context of the question \"Which school information are you given in the document?\", the answer is:\n",
            "\n",
            "* The school information provided in the document is related to the department of physics at a university.\n",
            "* The document provides details about the faculty members, including their names, qualifications, and experience.\n",
            "* It also lists the facilities available in the research center within the department.\n",
            "\n",
            "Therefore, the school information given in the document is related to the physics department of a university."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The document provided contains information about the department of physics at a university, including the names and qualifications of the faculty members, their experience, and the facilities available in the research center.\\n\\nIn the context of the question \"Which school information are you given in the document?\", the answer is:\\n\\n* The school information provided in the document is related to the department of physics at a university.\\n* The document provides details about the faculty members, including their names, qualifications, and experience.\\n* It also lists the facilities available in the research center within the department.\\n\\nTherefore, the school information given in the document is related to the physics department of a university.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zM5hibNTZcSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWAD9Y09aQSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}